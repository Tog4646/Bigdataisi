{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Projet Final Apache Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Nom Etudiant :*TOGO*  \n",
    "\n",
    "**Prenom Etudiant:*EMILE NARE*  \n",
    "\n",
    "**Classe :*MASTERE 1*  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Description\n",
    "Ce projet consiste à utiliser Apache Spark pour faire l'analyse et le traitement des données de **[San Francisco Fire Department Calls ](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)** afin de fournir quelques KPI (*Key Performance Indicator*). Le **SF Fire Dataset** comprend les réponses aux appels de toutes les unités d'incendie. Chaque enregistrement comprend le numéro d'appel, le numéro d'incident, l'adresse, l'identifiant de l'unité, le type d'appel et la disposition. Tous les intervalles de temps pertinents sont également inclus. Étant donné que ce Dataset est basé sur les réponses et que la plupart des appels impliquent plusieurs unités, ainsi il existe plusieurs enregistrements pour chaque numéro d'appel. Les adresses sont associées à un numéro de bloc, à une intersection ou à une boîte d'appel, et non à une adresse spécifique.\n",
    "\n",
    "**Plus de details sur la description des données cliquer sur ce [lien](https://data.sfgov.org/Public-Safety/Fire-Department-Calls-for-Service/nuek-vuh3)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Travail à faire.\n",
    "L'objectif de ce projet est de comprendre le **SF Fire Dataset** afin de bien répondre aux questions en utilisant les codes Spark/Scala adéquates.\n",
    "\n",
    "- Créer un repos git (public) et partager le repos avec mon mail (limahin10@gmail.com)\n",
    "- Ecrire un code lisible et bien indenté \n",
    "- N'oublier pas de mettre en commentaire la justification de vos réponses sur les cellules Markdown. \n",
    "\n",
    "\n",
    "## Note:\n",
    "- Le projet est personnel, c'est-à-dire chaque notebook ne concerne qu'un seul étudiant. \n",
    "- Deadline : **Lundi 31 Janvier 2022  à 23h 59** (Aucune de dérogation ne sera acceptée)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.types._ \n",
    "import org.apache.spark.sql.functions._ \n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nous allons jeter un coup d'oeil sur la structure des données avant de définir un schéma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vu que la taille de ces données est énormes, inferer le schema pour un très grande volumes de données s'avère un peu couteux. Nous allons ainsi définir un schema pour le Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fireSchema = StructType(Array(StructField(\"CallNumber\", IntegerType, true),\n",
    "  StructField(\"UnitID\", StringType, true),\n",
    "  StructField(\"IncidentNumber\", IntegerType, true),\n",
    "  StructField(\"CallType\", StringType, true),                  \n",
    "  StructField(\"CallDate\", StringType, true),      \n",
    "  StructField(\"WatchDate\", StringType, true),\n",
    "  StructField(\"CallFinalDisposition\", StringType, true),\n",
    "  StructField(\"AvailableDtTm\", StringType, true),\n",
    "  StructField(\"Address\", StringType, true),       \n",
    "  StructField(\"City\", StringType, true),       \n",
    "  StructField(\"Zipcode\", IntegerType, true),       \n",
    "  StructField(\"Battalion\", StringType, true),                 \n",
    "  StructField(\"StationArea\", StringType, true),       \n",
    "  StructField(\"Box\", StringType, true),       \n",
    "  StructField(\"OriginalPriority\", StringType, true),       \n",
    "  StructField(\"Priority\", StringType, true),       \n",
    "  StructField(\"FinalPriority\", IntegerType, true),       \n",
    "  StructField(\"ALSUnit\", BooleanType, true),       \n",
    "  StructField(\"CallTypeGroup\", StringType, true),\n",
    "  StructField(\"NumAlarms\", IntegerType, true),\n",
    "  StructField(\"UnitType\", StringType, true),\n",
    "  StructField(\"UnitSequenceInCallDispatch\", IntegerType, true),\n",
    "  StructField(\"FirePreventionDistrict\", StringType, true),\n",
    "  StructField(\"SupervisorDistrict\", StringType, true),\n",
    "  StructField(\"Neighborhood\", StringType, true),\n",
    "  StructField(\"Location\", StringType, true),\n",
    "  StructField(\"RowID\", StringType, true),\n",
    "  StructField(\"Delay\", FloatType, true)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val sfFireFile = \"datasets/sf-fire/sf-fire-calls.csv\"\n",
    "val fireDF = spark\n",
    "  .read\n",
    "  .schema(fireSchema)\n",
    "  .option(\"header\", \"true\")\n",
    "  .csv(sfFireFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nous allons mettre en cache le Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.count("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.printSchema("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fireDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Filtrage des d'appels de type \"Medical Incident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 1\n",
    "**Combien de types d'appels distincts ont été passés ?**  \n",
    "Pour être sûr, il ne faut pas compter les valeurs «nulles» dans la colonne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fewFireDF = fireDF\n",
    "  .select(\"IncidentNumber\", \"AvailableDtTm\", \"CallType\") \n",
    "  .where($\"CallType\" =!= \"Medical Incident\")\n",
    "\n",
    "fewFireDF.show(5, false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the .select() transformation to yank out just the 'Call Type' column\n",
    "# Add the .distinct() transformation to keep only distinct rows\n",
    "# Finally call the show action, to show the first 10 rows.\n",
    "# The False below expands the ASCII column width to fit the full text in the output\n",
    "\n",
    "// Reponse 1\n",
    "/*\n",
    "\n",
    "fireServiceCallsDF.select('CallType').distinct().show(35, False)\n",
    "*/### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3\n",
    "\n",
    "**Trouver toutes les réponses ou les délais sont supérieurs à 5 minutes?**\n",
    "\n",
    "*Indication\n",
    "1. Renommer la colonne Delay -> ReponseDelayedinMins\n",
    "2. Retourner un nouveau DataFrame\n",
    "3. Afficher tous les appels où le temps de réponse à un site d'incendie a eu lieu après un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Quels types d'appels différents ont été passés au service d'incendie?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that .count() is actually a transformation here\n",
    "\n",
    "// Reponse 2\n",
    "/*\n",
    "display(fireServiceCallsDF.select('CallType').groupBy('CallType').count().orderBy(\"count\", ascending=False))\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 3\n",
    "\n",
    "**Trouver toutes les réponses ou les délais sont supérieurs à 5 minutes?**\n",
    "\n",
    "*Indication\n",
    "1. Renommer la colonne Delay -> ReponseDelayedinMins\n",
    "2. Retourner un nouveau DataFrame\n",
    "3. Afficher tous les appels où le temps de réponse à un site d'incendie a eu lieu après un retard de plus de 5 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Reponse 3\n",
    "val newFireDF = fireDF.withColumnRenamed(\"Delay\", \"ResponseDelayedinMins\")\n",
    "/*\n",
    "Completer le code\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Transformations des dates  \n",
    "Maintenant nous allons d'abord:  \n",
    "1. Transformer les dates de type String en Spark Timestamp afin que nous puissions effectuer des requêtes basées sur la date plus tard    \n",
    "2. Retourner le Dataframe transformée  \n",
    "3. Mettre en cache le nouveau DataFrame  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val fireTSDF = newFireDF\n",
    "  .withColumn(\"IncidentDate\", to_timestamp(col(\"CallDate\"), \"MM/dd/yyyy\")).drop(\"CallDate\") \n",
    "  .withColumn(\"OnWatchDate\", to_timestamp(col(\"WatchDate\"), \"MM/dd/yyyy\")).drop(\"WatchDate\") \n",
    "  .withColumn(\"AvailableDtTS\", to_timestamp(col(\"AvailableDtTm\"), \"MM/dd/yyyy hh:mm:ss a\")).drop(\"AvailableDtTm\")\n",
    "\n",
    "fireTSDF.cache()\n",
    "\n",
    "import org.apache.spark.sql.functions._\n",
    "import spark.sqlContext.implicits._\n",
    "\n",
    "val dfFata=data2.toDF(\"input_timestamp\")\n",
    "\n",
    "dfDate.withColumn(\"datetype_timestamp\",\n",
    "to_timestamp(col(\"input_timestamp\"),\"MM-dd-yyyy HH mm ss SSS\"))\n",
    "    .show(false)\n",
    "  \n",
    "\n",
    "//String to timestamps\n",
    "val data = Seq((\"2021-07-01 12:01:19.000\"),\n",
    "    (\"2021-06-24 12:01:19.000\"),\n",
    "    (\"2021-14-16 16:44:55.406\"),\n",
    "    (\"2021-14-16 16:50:59.406\"))\n",
    "val df=data.toDF(\"07-01-2021\")\n",
    "df.withColumn(\"MM/dd/yyyy_timestamp\",\n",
    "        to_timestamp(col(\"CallDate\")))\n",
    "  .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 4\n",
    "**Quels sont les types d'appels les plus courants?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5-a\n",
    "**Quels sont boites postaux rencontrés dans les appels les plus courants?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 5-a\n",
    "/*\n",
    "Ecrire ici votre code\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 5-a\n",
    "**Quels sont les quartiers de San Francisco dont les codes postaux sont 94102 et 94103?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 5-b\n",
    "/*\n",
    "%sql \n",
    "SELECT NeighborhoodDistrict, count(NeighborhoodDistrict) AS Neighborhood_Count \n",
    "FROM firecalls \n",
    "between number 94102 AND 94103\n",
    "GROUP BY NeighborhoodDistrict\n",
    "ORDER BY Neighborhood_Count DESC \n",
    "LIMIT 15\n",
    "\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 6\n",
    "**Determiner le nombre total d'appels, ainsi que la moyenne, le minimum et le maximum du temps de réponse des appels?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 6\n",
    "/*\n",
    "Ecrire ici votre code\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7-a\n",
    "**Combien d'années distinctes trouve t-on dans ce Dataset?**  \n",
    "Dans ce dataset nous avons des données comprises entre 2000-2018. Vous pouvez utilisez la fonction Spark `year()` pour les dates en Timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 7-a\n",
    "/*\n",
    "fireServiceCallsDF.printSchema()\n",
    "from pyspark.sql.functions import *\n",
    "from_pattern1 = 'MM/dd/yyyy'\n",
    "to_pattern1 = 'yyyy-MM-dd'\n",
    "\n",
    "from_pattern2 = 'MM/dd/yyyy hh:mm:ss aa'\n",
    "to_pattern2 = 'MM/dd/yyyy hh:mm:ss aa'\n",
    "\n",
    "\n",
    "fireServiceCallsTsDF = fireServiceCallsDF \\\n",
    "  .withColumn('CallDateTS', unix_timestamp(fireServiceCallsDF['CallDate'], from_pattern1).cast(\"timestamp\")) \\\n",
    "  .drop('CallDate') \\\n",
    "  .withColumn('WatchDateTS', unix_timestamp(fireServiceCallsDF['WatchDate'], from_pattern1).cast(\"timestamp\")) \\\n",
    "  .drop('WatchDate') \\\n",
    "  .withColumn('ReceivedDtTmTS', unix_timestamp(fireServiceCallsDF['ReceivedDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('ReceivedDtTm') \\\n",
    "  .withColumn('EntryDtTmTS', unix_timestamp(fireServiceCallsDF['EntryDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('EntryDtTm') \\\n",
    "  .withColumn('DispatchDtTmTS', unix_timestamp(fireServiceCallsDF['DispatchDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('DispatchDtTm') \\\n",
    "  .withColumn('ResponseDtTmTS', unix_timestamp(fireServiceCallsDF['ResponseDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('ResponseDtTm') \\\n",
    "  .withColumn('OnSceneDtTmTS', unix_timestamp(fireServiceCallsDF['OnSceneDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('OnSceneDtTm') \\\n",
    "  .withColumn('TransportDtTmTS', unix_timestamp(fireServiceCallsDF['TransportDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('TransportDtTm') \\\n",
    "  .withColumn('HospitalDtTmTS', unix_timestamp(fireServiceCallsDF['HospitalDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('HospitalDtTm') \\\n",
    "  .withColumn('AvailableDtTmTS', unix_timestamp(fireServiceCallsDF['AvailableDtTm'], from_pattern2).cast(\"timestamp\")) \\\n",
    "  .drop('AvailableDtTm') \n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 7-b\n",
    "**Quelle semaine de l'année 2018 a eu le plus d'appels d'incendie?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 7-b\n",
    "/*\n",
    "fireServiceCallsDF.printSchema()\n",
    "fireServiceCallsTsDF \\\n",
    "  .select(year('CallDateTS')).distinct() \\\n",
    "  .orderBy('week(CallDateTS)').show()\n",
    "\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 8\n",
    "**Quels sont les quartiers de San Francisco qui ont connu le pire temps de réponse en 2018?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 8\n",
    "/*\n",
    "%sql \n",
    "SELECT NeighborhoodDistrict, count(NeighborhoodDistrict) AS Neighborhood_Count \n",
    "FROM firecalls \n",
    "WHERE year(CallDateTS) == 2018\n",
    "GROUP BY NeighborhoodDistrict\n",
    "ORDER BY Neighborhood_Count DESC \n",
    "LIMIT 18\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 9\n",
    "\n",
    "**Comment stocker les données du Dataframe sous format de fichiers Parquet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 9\n",
    "/*\n",
    "%fs ls /tmp/fireServiceParquet/\n",
    "fireServiceCallsTsDF.write.format('parquet').save('/tmp/fireServiceParquet/')\n",
    "*/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Question 10\n",
    "**Comment relire les données stockée en format Parquet?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Reponse 10\n",
    "/*\n",
    "tempDF = spark.read.parquet('/tmp/fireServiceParquet/')\n",
    "\n",
    "1) import pyarrow.parquet as pq\n",
    "\n",
    "path = 'parquet/part-r-00000-1e638be4-e31f-498a-a359-47d017a0059c.gz.parquet'\n",
    "table = pq.read_table(path)\n",
    "df = table.to_pandas()\n",
    "\n",
    "2)import pyarrow.parquet as pq\n",
    "\n",
    "dataset = pq.ParquetDataset('parquet/')\n",
    "table = dataset.read()\n",
    "df = table.to_pandas()\n",
    "\n",
    "*/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1f3c78614dfd3a9284edc7b0f59368844548d34f482c7a5df0a386665ec6b156"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
